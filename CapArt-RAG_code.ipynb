{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9dc6591",
   "metadata": {},
   "source": [
    "LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f65ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta del PDF.\n",
    "ruta_IVDR = r\"C:/Users/Alejandro/Desktop/IVDR.pdf\"\n",
    "\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "# Cargar el documento PDF y eliminar las dos 칰ltimas p치ginas sin contenido relevante (칤ndice).\n",
    "IVDR = PDFPlumberLoader(ruta_IVDR).load()\n",
    "documento_sin_final = IVDR[:-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093e65e7",
   "metadata": {},
   "source": [
    "CHUNKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbfbda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "# Combinar contenido (datos + metadatos) de las p치ginas en un solo documento. Sino, al dividir por cap칤tulos y art칤culos, se cortar칤a en cada final de p치gina.\n",
    "contenido_combinado = \"\\n\\n\".join(doc.page_content for doc in documento_sin_final)\n",
    "documento_combinado = Document(page_content=contenido_combinado, metadata=IVDR[0].metadata)\n",
    "\n",
    "import re\n",
    "# Expresi칩n regular para detectar cap칤tulos del estilo \"CAP칈TULO XX\".\n",
    "patron_capitulos = r\"(CAP칈TULO\\s+[IVXLCDM\\d]+(?:\\n[^\\n]+)?)\"\n",
    "division_capitulos = re.split(patron_capitulos, documento_combinado.page_content)\n",
    "\n",
    "# Construcci칩n de cap칤tulos como documentos. Al principio se a침ade el cap칤tulo 0 (Introducci칩n). Se coge el titulo del cap칤tulo y su contenido.\n",
    "capitulos = [(\"CAP칈TULO 0 - Introducci칩n\", division_capitulos[0].strip())] if division_capitulos[0].strip() else []\n",
    "\n",
    "for i, numero_capitulo in enumerate(range(1, len(division_capitulos) // 2 + 1), start=1):\n",
    "    titulo_capitulo = division_capitulos[i * 2 - 1].strip().replace('\\n', ' ')\n",
    "    titulo = f\"CAP칈TULO {numero_capitulo} - {titulo_capitulo}\"\n",
    "    contenido = f\"{titulo_capitulo}\\n\\n{division_capitulos[i * 2].strip()}\"\n",
    "    capitulos.append((titulo, contenido))\n",
    "\n",
    "# Crear documentos por cap칤tulo.\n",
    "documento_por_capitulos = [Document(page_content=contenido, metadata={\"titulo\": titulo}) for titulo, contenido in capitulos]\n",
    "\n",
    "from langchain_text_splitters import TextSplitter\n",
    "# Clase personalizada para dividir en art칤culos. Divide el texto en art칤culos, incluyendo la introducci칩n si existe.\n",
    "class ArticleTextSplitter(TextSplitter):\n",
    "    \n",
    "    def split_text(self, text: str):\n",
    "        pattern = r\"(Art칤culo\\s+\\d+\\n\\b[^\\n]*)\"\n",
    "        split_text = re.split(pattern, text)\n",
    "\n",
    "        articulos = []\n",
    "\n",
    "        # Si hay texto antes del primer 'Art칤culo', se considera introducci칩n.\n",
    "        if split_text[0].strip():\n",
    "            articulos.append({\n",
    "                \"titulo\": \"INTRODUCCI칍N\",\n",
    "                \"contenido\": split_text[0].strip()\n",
    "            })\n",
    "\n",
    "        for i in range(1, len(split_text), 2):\n",
    "            titulo_articulo = split_text[i].strip().replace('\\n', ' ')\n",
    "            contenido = f\"{titulo_articulo}\\n\\n{split_text[i + 1].strip()}\" if i + 1 < len(split_text) else titulo_articulo\n",
    "\n",
    "            articulos.append({\n",
    "                \"titulo\": titulo_articulo,\n",
    "                \"contenido\": contenido\n",
    "            })\n",
    "\n",
    "        return articulos\n",
    "\n",
    "# Divisi칩n por art칤culos.\n",
    "documento_por_articulos = ArticleTextSplitter()\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# Fragmentaci칩n en chunks para dividir art칤culos largos.\n",
    "chunk_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=10000,\n",
    "    chunk_overlap=500,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    ")\n",
    "\n",
    "# Lista final de chunks: Se divide por cap칤tulos, luego por art칤culos y finalmente en fragmentos m치s peque침os.\n",
    "chunks = []\n",
    "\n",
    "for capitulo in documento_por_capitulos:\n",
    "    articulos = documento_por_articulos.split_text(capitulo.page_content)\n",
    "\n",
    "    for articulo in articulos:\n",
    "        sub_chunks = chunk_splitter.split_text(articulo[\"contenido\"])\n",
    "\n",
    "        for i, sub_chunk in enumerate(sub_chunks):\n",
    "            chunks.append(\n",
    "                Document(\n",
    "                    page_content=sub_chunk,\n",
    "                    metadata={\n",
    "                        \"capitulo\": capitulo.metadata[\"titulo\"],\n",
    "                        \"art칤culo\": articulo[\"titulo\"],\n",
    "                        \"fragmento\": i  #칈ndice del fragmento dentro del art칤culo.\n",
    "                    }\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fc34f6",
   "metadata": {},
   "source": [
    "---LIMPIEZA (OPCIONAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22e00c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# 游늷 1. Eliminar por completo la base de datos anterior\n",
    "persist_directory = \"vectorstore\"\n",
    "shutil.rmtree(persist_directory, ignore_errors=True)  # Borra la carpeta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a9dd71",
   "metadata": {},
   "source": [
    "EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9e53383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear embeddings \"sentence-transformers/all-MiniLM-L6-v2\" vs \"BAAI/llm-embedder\".\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "modelo_embedding = HuggingFaceEmbeddings(model_name=\"BAAI/llm-embedder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a376848",
   "metadata": {},
   "source": [
    "VECTORSTORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343635fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la base de datos vectorial (vectorstore).\n",
    "from langchain_community.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    collection_name=\"BAAI_RAG\",\n",
    "    embedding=modelo_embedding)  #Si da algun error, borrarla con vectorstore.delete_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00af146b",
   "metadata": {},
   "source": [
    "---PRUEBAS RETRIEVE (OPCIONAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf8f31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pregunta = \"Clasificaci칩n de los productos sanitarios\"\n",
    "#-------PRUEBA DE COMO NOS SALDRIAN LAS RESPUESTAS-----------\n",
    "# Recuperar los documentos de la vectorstore m치s relevantes por similaridad\n",
    "test_docs = vectorstore.similarity_search_with_score(pregunta, k=10)\n",
    "\n",
    "# Ordenar manualmente por score de menor a mayor (los m치s relevantes primero)\n",
    "test_docs.sort(key=lambda x: x[1])\n",
    "\n",
    "# Mostrar los documentos ordenados con sus metadatos y scores\n",
    "for i, (doc, score) in enumerate(test_docs):\n",
    "    articulo = doc.metadata.get(\"art칤culo\", \"Desconocida\")  \n",
    "    title = doc.metadata.get(\"capitulo\", \"Sin t칤tulo\") \n",
    "    print(f\"Chunk {i+1} (Score: {score}, Art칤culo: {articulo}, T칤tulo: {title}):\\n{doc.page_content[:1300]}\\n\")\n",
    "#--------FIN DE LA PRUEBA, ELEGIR EL NUMERO DE K QUE MAS NOS INTERESE-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa328d45",
   "metadata": {},
   "source": [
    "RETRIEVING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7e5fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# La base de datos vectorial se utilizara como recuperador (retriever) de documentos.\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242696fa",
   "metadata": {},
   "source": [
    "HyDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe9bf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Se define un prompt para HyDE (Hyphotetical Document Embedding).\n",
    "HyDE_plantilla = \"\"\"Genera un texto regulatorio breve que podr칤a responder a la siguiente consulta. \n",
    "La respuesta debe ser concisa y estructurada en un m치ximo de 6 frases.\n",
    "Pregunta: {pregunta}  \n",
    "Texto:\"\"\"\n",
    "prompt_HyDE = ChatPromptTemplate.from_template(HyDE_plantilla)\n",
    "\n",
    "# Se define el LLM local a traves de Ollama que se utilizar치 para generar el documento hipot칠tico y la respuesta a la pregunta.\n",
    "llm = ChatOllama(model=\"llama3.2:3b\", temperature=0) # t = 0 para respuestas m치s precisas y menos creativas.\n",
    "\n",
    "# Cadena que une el prompt, el LLM y el parser de salida del documento hipot칠tico.\n",
    "cadena_generacion_HyDE = ( prompt_HyDE | llm | StrOutputParser())\n",
    "\n",
    "# Invocar la cadena de generaci칩n de documento HyDE solo con la pregunta.\n",
    "cadena_generacion_HyDE.invoke({\"pregunta\":pregunta})\n",
    "\n",
    "# Se define una cadena de recuperacion de documentos. \n",
    "cadena_recuperacion = cadena_generacion_HyDE | retriever \n",
    "\n",
    "# Invocar la cadena final solo con la pregunta.\n",
    "documentos_recuperados = cadena_recuperacion.invoke(pregunta)\n",
    "\n",
    "# Recuperar documentos relevantes\n",
    "documentos_recuperados = retriever.invoke(pregunta)\n",
    "\n",
    "# Funci칩n para concatenar documentos separ치ndolos por dos saltos de l칤nea.\n",
    "def concatenar_docs(documents):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "\n",
    "# Se concatenan los documentos antes de pasarlos al LLM.\n",
    "docs_concatenados = concatenar_docs(documentos_recuperados)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db97581",
   "metadata": {},
   "source": [
    "GENERACI칍N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b24dd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETAPA DE GENERACI칍N. Cadena de prompt(docs + pregunta) -> LLM -> Respuesta en string\n",
    "# Se define el prompt \n",
    "from langchain.prompts import PromptTemplate\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"Eres un asistente experto en regulaci칩n sanitaria que responde preguntas 칰nicamente utilizando el contenido proporcionado en los documentos normativos.\n",
    "\n",
    "Instrucciones importantes:\n",
    "- Solo puedes usar la informaci칩n incluida en los documentos para responder. No inventes ni asumas informaci칩n externa.\n",
    "- Si alg칰n fragmento menciona un art칤culo o cap칤tulo, debe citarse tambi칠n en la respuesta.\n",
    "- Si los documentos no contienen la informaci칩n suficiente para responder, simplemente indica que no dispones de esa informaci칩n.\n",
    "- La respuesta debe ser clara, directa y tener como m치ximo cuatro l칤neas.\n",
    "\n",
    "Pregunta: {pregunta}\n",
    "\n",
    "Documentos recuperados:\n",
    "{docs_concatenados}\n",
    "\n",
    "Respuesta:\"\"\",\n",
    "    input_variables=[\"pregunta\", \"docs_concatenados\"],\n",
    ")\n",
    "\n",
    "# Cadena final que une el prompt, el LLM y el parser de salida.\n",
    "cadena_generacion_rag = (prompt | llm | StrOutputParser())\n",
    "\n",
    "# Invocar la cadena final con los documentos formateados y la pregunta.\n",
    "cadena_generacion_rag.invoke({\"docs_concatenados\":docs_concatenados,\"pregunta\":pregunta})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
